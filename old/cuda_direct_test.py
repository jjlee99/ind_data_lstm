#!/usr/bin/env python3
"""
PyTorch ì—†ì´ ì§ì ‘ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import ctypes
import os
import sys

def test_cuda_library_direct():
    """CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì§ì ‘ ë¡œë“œí•´ì„œ í…ŒìŠ¤íŠ¸"""
    print("=== ì§ì ‘ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸ ===")
    
    try:
        # libcuda.so ì§ì ‘ ë¡œë“œ
        cuda_lib = ctypes.CDLL('libcuda.so.1')
        print("âœ… libcuda.so.1 ë¡œë“œ ì„±ê³µ")
        
        # cuInit í•¨ìˆ˜ í˜¸ì¶œ
        cu_init = cuda_lib.cuInit
        cu_init.argtypes = [ctypes.c_uint]
        cu_init.restype = ctypes.c_int
        
        result = cu_init(0)
        if result == 0:  # CUDA_SUCCESS
            print("âœ… CUDA ì´ˆê¸°í™” ì„±ê³µ")
            
            # GPU ê°œìˆ˜ í™•ì¸
            cu_device_get_count = cuda_lib.cuDeviceGetCount
            cu_device_get_count.argtypes = [ctypes.POINTER(ctypes.c_int)]
            cu_device_get_count.restype = ctypes.c_int
            
            device_count = ctypes.c_int()
            result = cu_device_get_count(ctypes.byref(device_count))
            
            if result == 0:
                print(f"âœ… GPU ê°œìˆ˜: {device_count.value}")
                return True
            else:
                print(f"âŒ GPU ê°œìˆ˜ í™•ì¸ ì‹¤íŒ¨: {result}")
        else:
            print(f"âŒ CUDA ì´ˆê¸°í™” ì‹¤íŒ¨: {result}")
            
    except Exception as e:
        print(f"âŒ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    return False

def test_environment_fix():
    """í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ PyTorch ì¬í…ŒìŠ¤íŠ¸"""
    print("\n=== í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í›„ PyTorch í…ŒìŠ¤íŠ¸ ===")
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    os.environ['CUDA_HOME'] = '/usr/local/cuda'
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # LD_LIBRARY_PATH ì„¤ì •
    current_ld_path = os.environ.get('LD_LIBRARY_PATH', '')
    new_paths = [
        '/usr/local/cuda/lib64',
        '/usr/local/cuda/targets/x86_64-linux/lib',
        '/usr/lib/x86_64-linux-gnu'
    ]
    
    all_paths = new_paths + ([current_ld_path] if current_ld_path else [])
    os.environ['LD_LIBRARY_PATH'] = ':'.join(all_paths)
    
    print(f"â†’ CUDA_HOME: {os.environ['CUDA_HOME']}")
    print(f"â†’ CUDA_VISIBLE_DEVICES: {os.environ['CUDA_VISIBLE_DEVICES']}")
    print(f"â†’ LD_LIBRARY_PATH: {os.environ['LD_LIBRARY_PATH']}")
    
    # PyTorch ë‹¤ì‹œ import (ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ íš¨ê³¼ì )
    print("\nâš ï¸  í™˜ê²½ ë³€ìˆ˜ ë³€ê²½ í›„ì—ëŠ” Pythonì„ ì¬ì‹œì‘í•´ì•¼ íš¨ê³¼ì ì…ë‹ˆë‹¤.")
    print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ìƒˆ Python ì„¸ì…˜ì—ì„œ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”:")
    print("python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"")

def check_container_nvidia_setup():
    """ì»¨í…Œì´ë„ˆì˜ NVIDIA ì„¤ì • í™•ì¸"""
    print("\n=== ì»¨í…Œì´ë„ˆ NVIDIA ì„¤ì • í™•ì¸ ===")
    
    # nvidia-ml-py3 ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
    try:
        import pynvml
        pynvml.nvmlInit()
        device_count = pynvml.nvmlDeviceGetCount()
        print(f"âœ… pynvmlì„ í†µí•œ GPU ê°œìˆ˜: {device_count}")
        
        if device_count > 0:
            handle = pynvml.nvmlDeviceGetHandleByIndex(0)
            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
            print(f"âœ… GPU ì´ë¦„: {name}")
            return True
            
    except ImportError:
        print("â†’ pynvml ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
        print("ì„¤ì¹˜: pip install pynvml")
    except Exception as e:
        print(f"âŒ pynvml í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ” CUDA ì§ì ‘ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # 1. ì§ì ‘ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ í…ŒìŠ¤íŠ¸
    cuda_direct_ok = test_cuda_library_direct()
    
    # 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    test_environment_fix()
    
    # 3. ì»¨í…Œì´ë„ˆ NVIDIA ì„¤ì • í™•ì¸
    nvidia_ok = check_container_nvidia_setup()
    
    print("\n=== í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ===")
    print(f"ì§ì ‘ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬: {'âœ…' if cuda_direct_ok else 'âŒ'}")
    print(f"NVIDIA ë¼ì´ë¸ŒëŸ¬ë¦¬: {'âœ…' if nvidia_ok else 'âŒ'}")
    
    if cuda_direct_ok:
        print("\nğŸ’¡ CUDA ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ì •ìƒì´ë¯€ë¡œ í™˜ê²½ ì„¤ì • ë¬¸ì œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë“¤ì„ ì‹¤í–‰í•´ë³´ì„¸ìš”:")
        print("1. export CUDA_HOME=/usr/local/cuda")
        print("2. export LD_LIBRARY_PATH=/usr/local/cuda/lib64:/usr/local/cuda/targets/x86_64-linux/lib:$LD_LIBRARY_PATH")
        print("3. export CUDA_VISIBLE_DEVICES=0")
        print("4. python -c \"import torch; print('CUDA available:', torch.cuda.is_available())\"")
    else:
        print("\nğŸ’¡ ì»¨í…Œì´ë„ˆê°€ GPUì— ì œëŒ€ë¡œ ì ‘ê·¼í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        print("ì»¨í…Œì´ë„ˆ ì‹¤í–‰ ì‹œ --gpus all ì˜µì…˜ì´ë‚˜ NVIDIA Container Runtimeì´ í•„ìš”í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()